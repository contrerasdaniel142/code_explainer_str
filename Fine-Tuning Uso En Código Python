{
  "runSettings": {
    "temperature": 1.0,
    "model": "models/gemini-2.5-pro-preview-06-05",
    "topP": 0.95,
    "topK": 64,
    "maxOutputTokens": 65536,
    "safetySettings": [{
      "category": "HARM_CATEGORY_HARASSMENT",
      "threshold": "OFF"
    }, {
      "category": "HARM_CATEGORY_HATE_SPEECH",
      "threshold": "OFF"
    }, {
      "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
      "threshold": "OFF"
    }, {
      "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
      "threshold": "OFF"
    }],
    "responseMimeType": "text/plain",
    "enableCodeExecution": false,
    "enableSearchAsATool": false,
    "enableBrowseAsATool": false,
    "enableAutoFunctionResponse": false
  },
  "citations": [{
    "uri": "https://github.com/B3nj4-u/etl-sales-analysis"
  }],
  "systemInstruction": {
  },
  "chunkedPrompt": {
    "chunks": [{
      "driveDocument": {
        "id": "18-AiEHdWHBedyP3GjMe1BwNOrVF2-xnZ"
      },
      "role": "user",
      "tokenCount": 14106
    }, {
      "text": "En que parte de este c√≥digo uso el modelo con fine tunning?",
      "role": "user",
      "tokenCount": 15
    }, {
      "text": "**Examining the Code\u0027s Use**\n\nI\u0027m currently focused on pinpointing where the fine-tuned model is being utilized within the code. I\u0027ve begun by meticulously dissecting the user\u0027s question, which seeks to identify the location of the model\u0027s application. I\u0027m carefully scrutinizing each line, aiming to precisely determine how the fine-tuned model functions within this framework.\n\n\n**Tracing Model Implementation**\n\nI\u0027ve made headway, having identified the `evaluate` function within the `training_utils.py` file as a crucial area. It indicates the *use* of the partially fine-tuned model for validation during the training loop. This is a critical step in assessing model performance, before training has completed. I am proceeding to further dissect the code to identify other locations of model utilization.\n\n\n**Unveiling Post-Training Use**\n\nI\u0027ve shifted my focus to the post-training utilization of the fine-tuned model. I have zeroed in on the `ModelComparator` class and the `run_model_comparison` function. This is where the fully fine-tuned model is employed to analyze performance and compare results with other models, providing a comprehensive assessment after training is complete.\n\n\n**Detailing Post-Training Application**\n\nI\u0027m now focusing on the post-training application of the fine-tuned model. I have identified the `run_model_comparison` function within the `model_comparator.py` module as a key area. It orchestrates the analysis of the fine-tuned model\u0027s performance in comparison to other models. This function is called at the end of the `train_distributed` function, indicating its execution *after* the training process is complete, using the *fully* fine-tuned model. I\u0027ll need to review the `ModelComparator` class and associated methods to understand the specific operations performed.\n\n\n**Uncovering Post-Training Validation**\n\nI\u0027m now focusing on the post-training validation process. I\u0027ve located the crucial `evaluate` function, which is nested within the `train_distributed` loop and called by the `CodeExplainerTrainer` class. This function performs evaluations using the model during training. The code snippet will need to be reviewed to determine how the evaluations are used to monitor the training process.\n\n\n**Unveiling Post-Training Use**\n\nI\u0027ve examined the `CodeExplainerTrainer` class and the `train_distributed` function. Specifically, I\u0027ve pinpointed the `run_model_comparison` function *after* the `train_distributed` loop as the place where the *fully* fine-tuned model is utilized for performance analysis. This is crucial for verifying the training loop\u0027s results and assessing the fine-tuned model\u0027s capabilities in comparison to other models.\n\n\n**Tracing Key Model Uses**\n\nI\u0027m zeroing in on the `ModelComparator` class, specifically, the `load_models` and `run_model_comparison` methods. The `load_models` method is critical because it loads and configures the fine-tuned model for performance comparison. I\u0027m focusing on dissecting how the `PeftModel` is used to load and incorporate the LoRA adapters into the base model.\n\n\n**Identifying Model Usage**\n\nI\u0027ve determined that the fine-tuned model is employed in two principal areas: evaluation during the training phase, specifically within the `evaluate` function, and post-training analysis through the `run_model_comparison` function. The former provides feedback during training, while the latter assesses performance after completion.\n\n\\n\\n**Refining Model Usage**\n\nI am now focusing on the precise details of model usage. During training, the partially fine-tuned model\u0027s performance is gauged in the `evaluate` function within the `CodeExplainerTrainer` class. After training, the fully fine-tuned model undergoes a comprehensive performance comparison through the `ModelComparator` and `run_model_comparison` functions.\n\n\\n\\n**Zeroing in on Key Function**\n\nI\u0027m now zeroing in on the `ModelComparator` and the `run_model_comparison` functions, which are critical components in post-training performance analysis. The `run_model_comparison` function within the `model_comparator.py` module orchestrates the evaluation of the fine-tuned model. This function is triggered post-training through a call within the `train_distributed` function, using the `fully` fine-tuned model.\n\n\\n\\n**Dissecting Model Implementation**\n\nI\u0027m now dissecting the inner workings of the `ModelComparator` class, especially the `load_models` method. This method is instrumental in loading and configuring the fine-tuned model for analysis. Specifically, it uses the `PeftModel` to incorporate the LoRA adapters into the base model. The `tuned_model` is constructed by combining the base model with the LoRA adapters, which were saved during the fine-tuning.\n\n\\n\\n**Delving into Model Loading**\n\nI\u0027ve zeroed in on the `load_models` method within the `ModelComparator` class. The critical step here is how the LoRA adapters are loaded and applied to the base model using `PeftModel.from_pretrained`. This reveals the mechanism by which the fine-tuned model, including its learned LoRA adapters, is integrated for the post-training analysis. The base model is loaded first, then `PeftModel` loads the adapters.\n\n\n**Analyzing Model Use Cases**\n\nI\u0027m presently focusing on how the fine-tuned model is employed within this codebase. During the training phase, the `evaluate` function within the `CodeExplainerTrainer` class is key. Subsequently, the fully fine-tuned model\u0027s performance is assessed and compared using the `ModelComparator` class, specifically through the `run_model_comparison` function within the `train_distributed` function.\n\n\n",
      "role": "model",
      "tokenCount": 2005,
      "isThought": true,
      "thoughtSignatures": ["", "", "", "", "", "", "", "", ""]
    }, {
      "text": "¬°Excelente pregunta! Analizando tu c√≥digo, el modelo con fine-tuning se utiliza principalmente en dos fases distintas:\n\n1.  **Durante el entrenamiento**: Para evaluar su rendimiento en el conjunto de validaci√≥n al final de cada √©poca.\n2.  **Despu√©s del entrenamiento**: En la clase `ModelComparator` para compararlo con el modelo base, generar un reporte y permitir una prueba interactiva.\n\nA continuaci√≥n, te detallo exactamente d√≥nde ocurre cada caso.\n\n---\n\n### 1. Durante el Entrenamiento (Evaluaci√≥n de Progreso)\n\nAl final de cada √©poca de entrenamiento, usas el modelo (que ha sido parcialmente \"fine-tuned\" hasta ese momento) para calcular la p√©rdida en el conjunto de datos de evaluaci√≥n. Esto te permite monitorear si el modelo est√° aprendiendo correctamente o si est√° sobreajustando.\n\nLa parte clave est√° en la funci√≥n `train_distributed`, dentro del bucle de √©pocas:\n\n```python\n# Dentro de la funci√≥n train_distributed, en el bucle de entrenamiento:\nfor epoch in range(config[\"num_epochs\"]):\n    # ... aqu√≠ ocurre el entrenamiento de una √©poca (trainer.train_epoch) ...\n    \n    # Despu√©s de entrenar, se usa el modelo para evaluar\n    process_logger.info(f\"Rank {rank}: Iniciando evaluaci√≥n para √âpoca {epoch+1}...\")\n    \n    # ¬°AQU√ç SE USA! -\u003e Se pasa el modelo fine-tuned (trainer.model.module) a la funci√≥n de evaluaci√≥n.\n    # Usamos .module para acceder al modelo original dentro del wrapper DDP.\n    eval_loss \u003d trainer.evaluate(trainer.model.module, trainer.eval_dataloader) \n    \n    if rank \u003d\u003d 0:\n        process_logger.info(f\"Evaluaci√≥n √âpoca {epoch+1} (global avg): Loss de validaci√≥n: {eval_loss:.4f}\")\n        # Despu√©s de evaluar, se guarda un checkpoint del modelo fine-tuned hasta ese punto\n        trainer.save_model(epoch\u003depoch)\n```\n\nEn este caso, el \"uso\" consiste en calcular la *loss* (p√©rdida) de validaci√≥n, no en generar texto.\n\n---\n\n### 2. Despu√©s del Entrenamiento (Comparaci√≥n y Uso Real)\n\nEsta es la parte m√°s importante y donde realmente ves los resultados del fine-tuning. La clase `ModelComparator` est√° dise√±ada espec√≠ficamente para esto.\n\n#### a) Carga del Modelo Fine-Tuned\n\nPrimero, el modelo que guardaste al final del entrenamiento (`final_model_path`) se carga. Como usaste PEFT (LoRA), no se carga un modelo completo, sino que se carga el modelo base y se le \"a√±aden\" los adaptadores (el resultado del fine-tuning).\n\nEsto ocurre en el m√©todo `ModelComparator.load_models()`:\n\n```python\n# Dentro de ModelComparator.load_models()\nself.logger.info(f\"Cargando modelo fine-tuned desde {self.tuned_model_path} en {self.device}...\")\n\n# Primero, se carga una instancia fresca del modelo base\nbase_for_peft \u003d AutoModelForCausalLM.from_pretrained(\n    self.base_model_id, \n    **base_for_peft_config\n)\n\n# ¬°AQU√ç SE CARGA EL MODELO FINE-TUNED!\n# PeftModel combina el modelo base con los adaptadores (LoRA) que fueron entrenados y guardados.\n# self.tuned_model_path es la ruta al checkpoint final del entrenamiento.\nself.tuned_model \u003d PeftModel.from_pretrained(\n    base_for_peft, \n    self.tuned_model_path, # La ruta donde se guard√≥ el modelo entrenado\n    is_trainable\u003dFalse\n)\n\n# ... se pone en modo de evaluaci√≥n\nself.tuned_model.eval()\n```\n\n#### b) Uso para Generar Texto y Evaluar M√©tricas\n\nUna vez cargado, `self.tuned_model` se usa para generar explicaciones de c√≥digo, que luego se comparan con las del modelo base.\n\nEsto sucede en dos lugares dentro de `ModelComparator`:\n\n**1. En la evaluaci√≥n autom√°tica sobre el conjunto de test (`evaluate_on_test_set`):**\n\n```python\n# Dentro de ModelComparator.evaluate_on_test_set\nfor i, example in enumerate(tqdm(test_samples, desc\u003d\"Generando predicciones\")):\n    # ...\n    # Se genera una respuesta con el modelo fine-tuned (`self.tuned_model`)\n    tuned_pred \u003d self.generate_response(self.tuned_model, prompt)\n    tuned_predictions.append(tuned_pred)\n    # ...\n```\n\n**2. En la comparaci√≥n interactiva (`interactive_comparison`):**\n\n```python\n# Dentro de ModelComparator.interactive_comparison\nwhile True:\n    # ...\n    print(\"\\nüî∏ MODELO FINE-TUNED:\")\n    print(\"-\"*40)\n    # Se genera una respuesta con el modelo fine-tuned a partir del input del usuario\n    tuned_response \u003d self.generate_response(self.tuned_model, prompt)\n    print(tuned_response)\n    # ...\n```\n\n### Resumen del Flujo Completo\n\n1.  `train_distributed` entrena el modelo y lo guarda en una ruta (`final_model_path`).\n2.  Al final de `train_distributed`, se llama a `run_model_comparison`, pas√°ndole la ruta del modelo reci√©n guardado.\n3.  `run_model_comparison` crea un `ModelComparator`.\n4.  `ModelComparator` carga el modelo fine-tuned (usando `PeftModel.from_pretrained`) en el atributo `self.tuned_model`.\n5.  Finalmente, `self.tuned_model` se utiliza para generar texto y medir su calidad, compar√°ndolo con el modelo original.",
      "role": "model",
      "finishReason": "STOP",
      "tokenCount": 1292
    }, {
      "text": "Ahora crea un script para desplegar ese modelo con streamlit, se lo mas profesional que puedas, crea varios archivos de ser necesario",
      "role": "user",
      "tokenCount": 26
    }, {
      "text": "**Examining Deployment Options**\n\nI\u0027m currently exploring the deployment landscape, narrowing my focus to Streamlit as a platform. My recent work involves deconstructing the deployment request. The central task is clear: deploying a PEFT/LoRA fine-tuned model via Streamlit. I\u0027m noting the model\u0027s causal architecture as a key factor in deployment considerations.\n\n\n**Prioritizing Code Structure**\n\nI\u0027ve shifted my focus to the architectural decisions needed for a \"professional\" deployment. This entails separating model loading from the UI, embracing configuration files, and leveraging `@st.cache_resource` for efficient model caching. Moreover, I\u0027m considering user experience improvements through clearer instructions and informative loading states.\n\n\n**Architecting the Backend**\n\nI\u0027m now deep-diving into the backend design. My goal is to build a robust model loading and inference function. It will be the central component that `app.py` relies upon. This function must handle loading the base model, applying the LoRA adapters, and managing device placement (CPU or GPU). It should also leverage `@st.cache_resource` for efficient caching, keeping the user experience snappy and responsive.\n\n\n**Structuring the Deployment**\n\nI\u0027ve sketched a preliminary directory structure, incorporating `app.py`, `model_loader.py`, `config.py`, `requirements.txt`, and a `README.md`. My focus now shifts to populating `config.py` with necessary parameters and defining the backend functions in `model_loader.py`. I\u0027m prioritizing efficient caching with Streamlit\u0027s tools, ensuring a streamlined user experience. The next step involves solidifying `requirements.txt` based on existing dependencies.\n\n\n**Developing the `model_loader`**\n\nI\u0027m now fully immersed in developing `model_loader.py`. I\u0027ve outlined the `load_model_and_tokenizer()` function, anticipating a clean integration with `@st.cache_resource`. The main focus is on loading the base model, applying the LoRA adapters using PEFT, and optimizing device placement. I am still investigating how to handle quantized models and how to set the device. My aim is a robust and efficient solution.\n\n\n**Implementing Caching Strategies**\n\nI\u0027m currently focused on the efficient caching of the model. I\u0027ve successfully integrated `@st.cache_resource` in `model_loader.py` to ensure the model loads only once. The next step is optimizing the `generate_explanation()` function, focusing on the prompt construction and generation parameters for robust and responsive text generation. The structure of the `config.py` file is complete.\n\n\n**Implementing the Inference Logic**\n\nI\u0027m currently focused on the core inference logic within `model_loader.py`. I\u0027m building out `generate_explanation()` to handle prompt construction, tokenization, and model inference. Prompt engineering is crucial, so I\u0027m paying close attention to replicating the original training prompt. I\u0027m also ensuring that generation parameters are configurable within `app.py` via the Streamlit UI. The final step is testing for the device\u0027s stability.\n\n\n**Developing Inference Logic**\n\nI\u0027ve successfully set up the backend and now I\u0027m building out the `generate_explanation` function. I\u0027m focused on the prompting strategy, making sure the model receives the correct instructions and context. The goal is to accurately reproduce the training prompt template for the best results. Moreover, I\u0027m integrating user-adjustable generation parameters through the Streamlit UI, with appropriate error handling and informational messages for users. This will be the heart of the app.\n\n\n**Optimizing Inference Flow**\n\nI\u0027m currently focused on the prompt construction and inference function within `model_loader.py`. I\u0027ve made significant progress in defining `generate_explanation()` to handle the full workflow. This function will now accept user input, construct the prompt, tokenize it, and pass it to the model for generation. Moreover, I\u0027m integrating user-settable generation parameters from `app.py`, enhancing app flexibility and user experience.\n\n\n**Optimizing Backend Design**\n\nI\u0027m now fully immersed in the backend architecture. My current focus is refining the `model_loader.py` file, centralizing model loading, inference logic, and device management. I am ensuring that the `generate_explanation()` function seamlessly integrates with the Streamlit UI, accepting user input and generation parameters. It\u0027s key that the prompt is correct.\n\n\n",
      "role": "model",
      "tokenCount": 2299,
      "isThought": true,
      "thoughtSignatures": ["", "", "", "", "", "", "", "", "", ""]
    }, {
      "text": "¬°Absolutamente! Crear una aplicaci√≥n profesional con Streamlit implica separar la l√≥gica, gestionar la configuraci√≥n y asegurar una buena experiencia de usuario. Aqu√≠ te presento una estructura de proyecto completa y bien documentada para desplegar tu modelo fine-tuned.\n\n### Estructura del Proyecto\n\nCrearemos la siguiente estructura de archivos para mantener el c√≥digo organizado y profesional:\n\n```\ncode-explainer-app/\n‚îú‚îÄ‚îÄ .streamlit/\n‚îÇ   ‚îî‚îÄ‚îÄ config.toml        # (Opcional) Configuraci√≥n de la apariencia de Streamlit\n‚îú‚îÄ‚îÄ app.py                 # L√≥gica de la interfaz de usuario con Streamlit\n‚îú‚îÄ‚îÄ config.py              # Archivo de configuraci√≥n para rutas y par√°metros\n‚îú‚îÄ‚îÄ model_loader.py        # L√≥gica para cargar el modelo y el tokenizador\n‚îú‚îÄ‚îÄ README.md              # Instrucciones de uso y configuraci√≥n\n‚îî‚îÄ‚îÄ requirements.txt       # Dependencias del proyecto\n```\n\n---\n\n### 1. Archivo de Configuraci√≥n (`config.py`)\n\nEste archivo centraliza todas las rutas y par√°metros importantes, facilitando la modificaci√≥n sin tener que tocar el c√≥digo de la aplicaci√≥n.\n\n```python\n# code-explainer-app/config.py\n\nimport torch\n\n# --- CONFIGURACI√ìN PRINCIPAL ---\n# Cambia estos valores seg√∫n tu configuraci√≥n\n\n# ID del modelo base que usaste para el fine-tuning (ej. \"NousResearch/Llama-2-7b-chat-hf\")\nBASE_MODEL_ID \u003d \"codellama/CodeLlama-7b-Instruct-hf\" \n\n# Ruta al directorio donde guardaste los adaptadores LoRA (tu modelo fine-tuned)\n# Por ejemplo: \"/path/to/your/project/output/final_model\"\nTUNED_MODEL_PATH \u003d \"ruta/a/tu/modelo/final\" \n\n# --- CONFIGURACI√ìN DE INFERENCIA ---\n\n# Device para la inferencia. \"cuda\" si tienes GPU, de lo contrario \"cpu\"\nDEVICE \u003d \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Habilitar cuantizaci√≥n de 4 bits (BitsAndBytes) para ahorrar memoria en GPU.\n# No se usar√° si DEVICE es \"cpu\".\nUSE_4BIT_QUANTIZATION \u003d True\n\n# --- CONFIGURACI√ìN DE GENERACI√ìN ---\n\n# Template del prompt para la inferencia. Debe ser el mismo que usaste en la comparaci√≥n.\nPROMPT_TEMPLATE \u003d \"\"\"\u003c|system|\u003e\nYou are a programming expert who explains code clearly and concisely.\n\u003c|user|\u003e\nExplain this code:\n{code}\n\u003c|assistant|\u003e\n\"\"\"\n\n# Par√°metros por defecto para la generaci√≥n de texto\nDEFAULT_MAX_NEW_TOKENS \u003d 256\nDEFAULT_TEMPERATURE \u003d 0.7\n```\n\n---\n\n### 2. L√≥gica de Carga del Modelo (`model_loader.py`)\n\nEste m√≥dulo se encarga de la tarea pesada: cargar el modelo y el tokenizador. Usaremos el decorador `@st.cache_resource` de Streamlit para que el modelo se cargue una sola vez y permanezca en memoria, haciendo la aplicaci√≥n incre√≠blemente r√°pida despu√©s del arranque inicial.\n\n```python\n# code-explainer-app/model_loader.py\n\nimport torch\nimport streamlit as st\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig\n)\nfrom peft import PeftModel\nfrom config import (\n    BASE_MODEL_ID,\n    TUNED_MODEL_PATH,\n    DEVICE,\n    USE_4BIT_QUANTIZATION,\n    PROMPT_TEMPLATE\n)\n\ndef create_bnb_config() -\u003e BitsAndBytesConfig:\n    \"\"\"Crea la configuraci√≥n BitsAndBytes optimizada para inferencia.\"\"\"\n    return BitsAndBytesConfig(\n        load_in_4bit\u003dTrue,\n        bnb_4bit_use_double_quant\u003dTrue,\n        bnb_4bit_quant_type\u003d\"nf4\",\n        bnb_4bit_compute_dtype\u003dtorch.bfloat16 # Usar bfloat16 para inferencia es com√∫n\n    )\n\n@st.cache_resource\ndef load_model_and_tokenizer():\n    \"\"\"\n    Carga el modelo base, aplica los adaptadores PEFT y carga el tokenizador.\n    Utiliza st.cache_resource para evitar recargar el modelo en cada interacci√≥n.\n    \"\"\"\n    st.info(\"Cargando tokenizador...\")\n    tokenizer \u003d AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code\u003dTrue)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token \u003d tokenizer.eos_token\n\n    bnb_config \u003d None\n    if USE_4BIT_QUANTIZATION and DEVICE \u003d\u003d \"cuda\":\n        st.info(\"Cuantizaci√≥n de 4-bits habilitada. Creando configuraci√≥n BNB...\")\n        bnb_config \u003d create_bnb_config()\n\n    st.info(f\"Cargando modelo base \u0027{BASE_MODEL_ID}\u0027 en {DEVICE}...\")\n    base_model \u003d AutoModelForCausalLM.from_pretrained(\n        BASE_MODEL_ID,\n        quantization_config\u003dbnb_config,\n        device_map\u003d\"auto\", # \"auto\" maneja la distribuci√≥n en GPUs o CPU\n        torch_dtype\u003dtorch.bfloat16 if DEVICE \u003d\u003d \"cuda\" else torch.float32,\n        trust_remote_code\u003dTrue\n    )\n\n    st.info(f\"Cargando adaptadores LoRA desde \u0027{TUNED_MODEL_PATH}\u0027...\")\n    try:\n        tuned_model \u003d PeftModel.from_pretrained(base_model, TUNED_MODEL_PATH)\n        tuned_model.eval()\n        st.success(\"¬°Modelo fine-tuned cargado y listo para usar!\")\n        return tuned_model, tokenizer\n    except Exception as e:\n        st.error(f\"Error al cargar los adaptadores PEFT: {e}\")\n        st.warning(\"Se usar√° el modelo base sin fine-tuning.\")\n        base_model.eval()\n        return base_model, tokenizer\n\ndef generate_explanation(model, tokenizer, code: str, max_new_tokens: int, temperature: float) -\u003e str:\n    \"\"\"\n    Genera una explicaci√≥n para un fragmento de c√≥digo dado usando el modelo cargado.\n    \"\"\"\n    prompt \u003d PROMPT_TEMPLATE.format(code\u003dcode)\n    \n    inputs \u003d tokenizer(prompt, return_tensors\u003d\"pt\").to(DEVICE)\n    \n    with torch.no_grad():\n        outputs \u003d model.generate(\n            **inputs,\n            max_new_tokens\u003dmax_new_tokens,\n            temperature\u003dmax(temperature, 0.01), # Evitar temperatura de 0\n            do_sample\u003dTrue,\n            top_p\u003d0.9,\n            top_k\u003d50,\n            pad_token_id\u003dtokenizer.eos_token_id,\n            eos_token_id\u003dtokenizer.eos_token_id\n        )\n    \n    generated_tokens \u003d outputs[0][inputs[\u0027input_ids\u0027].shape[1]:]\n    response \u003d tokenizer.decode(generated_tokens, skip_special_tokens\u003dTrue)\n    \n    return response.strip()\n\n```\n\n---\n\n### 3. Interfaz de Usuario (`app.py`)\n\nEste es el punto de entrada de la aplicaci√≥n. Mantenemos este archivo limpio, enfoc√°ndonos solo en los elementos de la interfaz y llamando a las funciones de `model_loader.py`.\n\n```python\n# code-explainer-app/app.py\n\nimport streamlit as st\nfrom config import DEFAULT_MAX_NEW_TOKENS, DEFAULT_TEMPERATURE\nfrom model_loader import load_model_and_tokenizer, generate_explanation\n\n# --- Configuraci√≥n de la p√°gina ---\nst.set_page_config(\n    page_title\u003d\"Code Explainer AI\",\n    page_icon\u003d\"ü§ñ\",\n    layout\u003d\"wide\",\n    initial_sidebar_state\u003d\"expanded\"\n)\n\n# --- T√≠tulo y Descripci√≥n ---\nst.title(\"ü§ñ Code Explainer AI\")\nst.markdown(\"\"\"\nEsta aplicaci√≥n utiliza un modelo de lenguaje fine-tuned para generar explicaciones claras y concisas de fragmentos de c√≥digo. \nSimplemente pega tu c√≥digo en el √°rea de texto, ajusta los par√°metros si lo deseas y obt√©n una explicaci√≥n al instante.\n\"\"\")\n\n# --- Carga del Modelo (con spinner y cache) ---\nwith st.spinner(\"Cargando el modelo experto en c√≥digo... Esto puede tardar unos momentos.\"):\n    model, tokenizer \u003d load_model_and_tokenizer()\n\n# --- Barra Lateral (Sidebar) para Opciones ---\nst.sidebar.header(\"‚öôÔ∏è Configuraci√≥n de Generaci√≥n\")\n\nmax_new_tokens \u003d st.sidebar.slider(\n    \"Tokens m√°ximos de respuesta:\",\n    min_value\u003d64,\n    max_value\u003d1024,\n    value\u003dDEFAULT_MAX_NEW_TOKENS,\n    step\u003d64,\n    help\u003d\"El n√∫mero m√°ximo de tokens (palabras/s√≠mbolos) que el modelo generar√° en su respuesta.\"\n)\n\ntemperature \u003d st.sidebar.slider(\n    \"Temperatura:\",\n    min_value\u003d0.1,\n    max_value\u003d1.5,\n    value\u003dDEFAULT_TEMPERATURE,\n    step\u003d0.1,\n    help\u003d\"Controla la aleatoriedad de la respuesta. Valores m√°s bajos la hacen m√°s determinista, valores m√°s altos la hacen m√°s creativa.\"\n)\n\nst.sidebar.markdown(\"---\")\nst.sidebar.info(\"La aplicaci√≥n est√° construida con un modelo fine-tuned usando PEFT/LoRA.\")\n\n# --- √Årea Principal ---\nst.header(\"Explica mi C√≥digo\")\n\ncode_input \u003d st.text_area(\n    \"Pega tu c√≥digo aqu√≠:\",\n    height\u003d250,\n    placeholder\u003d\"def hello_world():\\n    print(\u0027Hello, World!\u0027)\",\n    key\u003d\"code_input\"\n)\n\nif st.button(\"Generar Explicaci√≥n\", type\u003d\"primary\", use_container_width\u003dTrue):\n    if not code_input.strip():\n        st.warning(\"Por favor, ingresa un fragmento de c√≥digo para explicar.\")\n    else:\n        with st.spinner(\"üß† Pensando en la mejor explicaci√≥n...\"):\n            try:\n                explanation \u003d generate_explanation(\n                    model,\n                    tokenizer,\n                    code_input,\n                    max_new_tokens,\n                    temperature\n                )\n                st.subheader(\"‚úÖ Explicaci√≥n Generada:\")\n                st.markdown(explanation)\n            except Exception as e:\n                st.error(f\"Ocurri√≥ un error durante la generaci√≥n: {e}\")\n\n```\n\n---\n\n### 4. Dependencias y Documentaci√≥n\n\n#### `requirements.txt`\n\nEste archivo es crucial para que otros puedan instalar y correr tu aplicaci√≥n f√°cilmente.\n\n```\n# code-explainer-app/requirements.txt\nstreamlit\ntorch\ntransformers\npeft\naccelerate\nbitsandbytes\nsentencepiece\neinops\n```\n*Nota: Aseg√∫rate de que tu `torch` est√© instalado con el soporte de CUDA correcto si usas una GPU.*\n\n#### `README.md`\n\nUn buen `README` es la tarjeta de presentaci√≥n de tu proyecto.\n\n```markdown\n# ü§ñ Code Explainer AI\n\nUna aplicaci√≥n web construida con Streamlit para generar explicaciones de c√≥digo utilizando un modelo de lenguaje fine-tuned con PEFT/LoRA.\n\n## Caracter√≠sticas\n\n-   **Interfaz Sencilla**: Pega tu c√≥digo y obt√©n una explicaci√≥n con un solo clic.\n-   **Configurable**: Ajusta la temperatura y la longitud m√°xima de la respuesta.\n-   **Eficiente**: El modelo se carga una sola vez gracias al cache de Streamlit, proporcionando respuestas r√°pidas.\n-   **Profesional**: Estructura de c√≥digo modular y f√°cil de mantener.\n\n## Requisitos Previos\n\n-   Python 3.8+\n-   Git\n-   (Opcional pero recomendado) Una GPU NVIDIA con CUDA para un rendimiento √≥ptimo.\n\n## ‚öôÔ∏è Configuraci√≥n\n\n1.  **Clona el repositorio:**\n    ```bash\n    git clone \u003cURL-del-repositorio\u003e\n    cd code-explainer-app\n    ```\n\n2.  **Crea y activa un entorno virtual (recomendado):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # En Windows: venv\\Scripts\\activate\n    ```\n\n3.  **Instala las dependencias:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n    *Si tienes una GPU, aseg√∫rate de que tu instalaci√≥n de PyTorch coincida con tu versi√≥n de CUDA. Visita [pytorch.org](https://pytorch.org/) para obtener el comando de instalaci√≥n correcto.*\n\n4.  **Configura tus modelos:**\n    -   Abre el archivo `config.py`.\n    -   Establece el `BASE_MODEL_ID` al identificador del modelo que usaste como base en Hugging Face.\n    -   **IMPORTANTE**: Actualiza `TUNED_MODEL_PATH` con la ruta absoluta o relativa al directorio donde guardaste los adaptadores LoRA de tu modelo fine-tuned.\n\n## üöÄ Ejecutar la Aplicaci√≥n\n\nUna vez configurado, inicia la aplicaci√≥n con el siguiente comando:\n\n```bash\nstreamlit run app.py\n```\n\nTu navegador se abrir√° autom√°ticamente en la direcci√≥n `http://localhost:8501`.\n\n## Estructura del Proyecto\n\n-   `app.py`: El script principal de Streamlit que define la interfaz de usuario.\n-   `config.py`: Archivo de configuraci√≥n central para rutas de modelos y par√°metros.\n-   `model_loader.py`: Contiene la l√≥gica para cargar el modelo y generar texto.\n-   `requirements.txt`: Lista de dependencias de Python.\n-   `.streamlit/config.toml`: (Opcional) Para personalizar el tema de Streamlit.\n```\n\n### 5. (Opcional) Personalizar la Apariencia (`.streamlit/config.toml`)\n\nPara un toque extra de profesionalismo, puedes definir un tema.\n\n```toml\n# code-explainer-app/.streamlit/config.toml\n\n[theme]\nprimaryColor\u003d\"#4F8BF9\"\nbackgroundColor\u003d\"#0E1117\"\nsecondaryBackgroundColor\u003d\"#262730\"\ntextColor\u003d\"#FAFAFA\"\nfont\u003d\"sans serif\"\n```\n\nCon esta estructura, tienes una aplicaci√≥n robusta, f√°cil de mantener y de aspecto profesional, lista para ser desplegada.",
      "role": "model",
      "finishReason": "STOP",
      "tokenCount": 3215
    }],
    "pendingInputs": [{
      "text": "",
      "role": "user"
    }]
  }
}